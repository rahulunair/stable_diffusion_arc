{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ca7ac3ac-5154-4082-a8a5-426b4a970eff",
   "metadata": {},
   "source": [
    "SPDX-License-Identifier: Apache-2.0\n",
    "Copyright (c) 2023, Rahul Unnikrishnan Nair <rahul.unnikrishnan.nair@intel.com>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f57b36-16a0-4908-bc09-e09af13bcaa3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Welcome to the Magic of Words! - Text-to-Image with Stable Diffusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95841e46-5430-4cb7-9733-de863e2ae321",
   "metadata": {},
   "source": [
    "Welcome! Whether you're a writer, designer, developer, or someone curious about the intersection of text and visuals, this guide is for you.\n",
    "\n",
    "Have you ever wanted to convert a description or phrase into an image? With Text-to-Image Stable Diffusion, you can do just that. Input a description like \"a quiet sunset over the ocean\" or \"a cityscape at night,\" and see it translated into a visual representation.\n",
    "\n",
    "This process is powered by Intel's Max Series GPUs. These GPUs are optimized for tasks like Text-to-Image Stable Diffusion, ensuring efficient and accurate conversions.\n",
    "\n",
    "This guide will walk you through examples, help you understand the outputs, and give pointers on how to get the best results. No technical deep dives, just clear steps and explanations.\n",
    "\n",
    "Ready to turn your words into visuals? Let's get started by setting up few things and importing the required python packages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee25d6c0-1c17-4497-bd74-f7795887d7e2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# install and import required dependencies\n",
    "# %pip install transformers accelerate diffusers pillow >/dev/nul\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for a cleaner output.\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex  # Used for optimizing PyTorch models\n",
    "from PIL import Image\n",
    "\n",
    "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
    "from io import BytesIO\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22619ef2-118d-41b0-ac46-9c80261cc138",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A Glimpse Behind the Scenes**\n",
    "\n",
    "For those intrigued by the underpinnings of this adventure, let's delve into the technicalities of the code. No worries if you're not aiming for a deep dive; understanding this isn't a prerequisite to harness the magic of the notebook. But for the tech-curious, let's dissect:\n",
    "\n",
    "- **Class Blueprint**: At the heart of our operations is the `Text2ImgModel` class. This acts as the central hub, choreographing the process of transforming textual prompts into visual masterpieces.\n",
    "\n",
    "- **Initialization**: When we instantiate this class, we have the liberty to tailor its behavior. This includes selecting the model's path, the device (defaulting to Intel's \"xpu\"), the data type, and more. There are also provisions to supercharge the model's performance via optimizations.\n",
    "\n",
    "- **Model Onboarding**: The `_load_pipeline` function is where we bring the pre-trained model onboard. This sets the stage for all our text-to-image transformations.\n",
    "\n",
    "- **Optimization**: Performance is paramount. With the `_optimize_pipeline` and `optimize_pipeline` methods, we leverage Intel-specific optimizations using the Intel Extension For PyTorch (IPEX) to ensure our model runs like the wind.\n",
    "\n",
    "- **Warming Up**: Just like a car on a cold morning, our model benefits from a brief warm-up. The `warmup_model` method handles this, ensuring that our model is primed and ready for the tasks ahead.\n",
    "\n",
    "- **The Grand Finale - Image Generation**: The `generate_images` method is where dreams meet reality. It interprets the textual prompts, consults with the model, and then crafts images that encapsulate the essence of the prompts. You can specify the number of images, the number of noise removal steps, and even the save path for these artworks.\n",
    "\n",
    "Intrigued? Dive into the code below and see how we've meticulously tailored every aspect, ensuring it's primed for Intel GPUs.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be12d985-31b5-4a03-b976-74787558fc0f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Text2ImgModel:\n",
    "    \"\"\"\n",
    "    Text2ImgModel is a class for generating images based on text prompts using a pretrained model.\n",
    "\n",
    "    Attributes:\n",
    "    - device: The device to run the model on. Default to \"xpu\" - Intel dGPUs.\n",
    "    - pipeline: The loaded model pipeline.\n",
    "    - data_type: The data type to use in the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id_or_path: str,\n",
    "        device: str = \"xpu\",\n",
    "        torch_dtype: torch.dtype = torch.bfloat16,\n",
    "        optimize: bool = True,\n",
    "        enable_scheduler: bool = False,\n",
    "        warmup: bool = True\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        The initializer for Text2ImgModel class.\n",
    "\n",
    "        Parameters:\n",
    "        - model_id_or_path: The identifier or path of the pretrained model.\n",
    "        - device: The device to run the model on. Default is \"xpu\".\n",
    "        - torch_dtype: The data type to use in the model. Default is torch.bfloat16.\n",
    "        - optimize: Whether to optimize the model after loading. Default is True.\n",
    "        \"\"\"\n",
    "\n",
    "        self.device = device\n",
    "        self.pipeline = self._load_pipeline(\n",
    "            model_id_or_path, torch_dtype, enable_scheduler\n",
    "        )\n",
    "        self.data_type = torch_dtype\n",
    "        if optimize:\n",
    "            start_time = time.time()\n",
    "            print(\"Optimizing the model...\")\n",
    "            self.optimize_pipeline()\n",
    "            print(\n",
    "                \"Optimization completed in {:.2f} seconds.\".format(\n",
    "                    time.time() - start_time\n",
    "                )\n",
    "            )\n",
    "        if warmup:\n",
    "            self.warmup_model()\n",
    "\n",
    "    def _load_pipeline(\n",
    "        self,\n",
    "        model_id_or_path: str,\n",
    "        torch_dtype: torch.dtype,\n",
    "        enable_scheduler: bool\n",
    "    ) -> DiffusionPipeline:\n",
    "        \"\"\"\n",
    "        Loads the pretrained model and prepares it for inference.\n",
    "\n",
    "        Parameters:\n",
    "        - model_id_or_path: The identifier or path of the pretrained model.\n",
    "        - torch_dtype: The data type to use in the model.\n",
    "\n",
    "        Returns:\n",
    "        - pipeline: The loaded model pipeline.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Loading the model...\")\n",
    "        pipeline = DiffusionPipeline.from_pretrained(\n",
    "            model_id_or_path,\n",
    "            torch_dtype=torch_dtype,\n",
    "            use_safetensors=True,\n",
    "            variant=\"fp16\",\n",
    "        )\n",
    "        if enable_scheduler:\n",
    "            pipeline.scheduler = DPMSolverMultistepScheduler.from_config(\n",
    "                pipeline.scheduler.config\n",
    "            )\n",
    "        pipeline = pipeline.to(self.device)\n",
    "        print(\"Model loaded.\")\n",
    "        return pipeline\n",
    "\n",
    "    def _optimize_pipeline(self, pipeline: DiffusionPipeline) -> DiffusionPipeline:\n",
    "        \"\"\"\n",
    "        Optimizes the model for inference using ipex.\n",
    "\n",
    "        Parameters:\n",
    "        - pipeline: The model pipeline to be optimized.\n",
    "\n",
    "        Returns:\n",
    "        - pipeline: The optimized model pipeline.\n",
    "        \"\"\"\n",
    "\n",
    "        for attr in dir(pipeline):\n",
    "            if isinstance(getattr(pipeline, attr), nn.Module):\n",
    "                setattr(\n",
    "                    pipeline,\n",
    "                    attr,\n",
    "                    ipex.optimize(\n",
    "                        getattr(pipeline, attr).eval(),\n",
    "                        dtype=pipeline.text_encoder.dtype,\n",
    "                        inplace=True,\n",
    "                    ),\n",
    "                )\n",
    "        return pipeline\n",
    "\n",
    "    def warmup_model(self):\n",
    "        \"\"\"\n",
    "        Warms up the model by generating a sample image.\n",
    "        \"\"\"\n",
    "        print(\"Setting up model...\")\n",
    "        start_time = time.time()\n",
    "        self.generate_images(\n",
    "            prompt=\"A beautiful sunset over the mountains\",\n",
    "            num_images=1,\n",
    "            save_path=\"/tmp\",\n",
    "        )\n",
    "        print(\n",
    "            \"Model is set up and ready! Warm-up completed in {:.2f} seconds.\".format(\n",
    "                time.time() - start_time\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def optimize_pipeline(self) -> None:\n",
    "        \"\"\"\n",
    "        Optimizes the current model pipeline.\n",
    "        \"\"\"\n",
    "\n",
    "        self.pipeline = self._optimize_pipeline(self.pipeline)\n",
    "\n",
    "    def generate_images(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        num_inference_steps: int = 50,\n",
    "        num_images: int = 5,\n",
    "        save_path: str = \"output\",\n",
    "    ) -> List[Image.Image]:\n",
    "        \"\"\"\n",
    "        Generates images based on the given prompt and saves them to disk.\n",
    "\n",
    "        Parameters:\n",
    "        - prompt: The text prompt to generate images from.\n",
    "        - num_inference_steps: Number of noise removal steps.\n",
    "        - num_images: The number of images to generate. Default is 5.\n",
    "        - save_path: The directory to save the generated images in. Default is \"output\".\n",
    "\n",
    "        Returns:\n",
    "        - images: A list of the generated images.\n",
    "        \"\"\"\n",
    "\n",
    "        images = []\n",
    "        for i in range(num_images):\n",
    "            with torch.xpu.amp.autocast(\n",
    "                enabled=True if self.data_type != torch.float32 else False,\n",
    "                dtype=self.data_type,\n",
    "            ):\n",
    "                image = self.pipeline(\n",
    "                    prompt=prompt,\n",
    "                    num_inference_steps=num_inference_steps,\n",
    "                    #negative_prompt=negative_prompt,\n",
    "                ).images[0]\n",
    "                if not os.path.exists(save_path):\n",
    "                    try:\n",
    "                        os.makedirs(save_path)\n",
    "                    except OSError as e:\n",
    "                        print(\"Failed to create directory\", save_path, \"due to\", str(e))\n",
    "                        raise\n",
    "            output_image_path = os.path.join(\n",
    "                save_path,\n",
    "                f\"{'_'.join(prompt.split()[:3])}_{i}.png\",\n",
    "            )\n",
    "            image.save(output_image_path)\n",
    "            images.append(image)\n",
    "        return images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f488b7f0-c622-4712-96d3-6ae6310ccfde",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Setting Up the Text-to-Image Interface**\n",
    "\n",
    "The following section is dedicated to creating an intuitive interface right within this notebook, providing a seamless experience to translate your textual prompts into captivating visuals.\n",
    "\n",
    "- **Model Selection**: Pick your desired pre-trained model from the available list.\n",
    "- **Prompt Input**: Enter your creative textual prompt, the muse for the image to be generated.\n",
    "- **Number of Images**: Specify the number of visual interpretations you'd like to see for your prompt.\n",
    "- **Enhancement**: Toggle an option to sprinkle some automatic enhancements to your prompt, potentially enriching the outcome.\n",
    "\n",
    "After setting your preferences, all it takes is a button click to witness the text's metamorphosis into imagery!\n",
    "\n",
    "Let's lay the groundwork for this experience:\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f41a449-ae30-4853-a2b5-d34b6cbdbc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mp_img\n",
    "import validators\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Image as IPImage\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def prompt_to_image():\n",
    "    out = widgets.Output()\n",
    "\n",
    "    output_dir = \"output\"\n",
    "    model_ids = [\n",
    "        \"CompVis/stable-diffusion-v1-4\",\n",
    "        \"stabilityai/stable-diffusion-2-1\",\n",
    "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    ]    \n",
    "    \n",
    "    model_dropdown = widgets.Dropdown(\n",
    "        options=model_ids,\n",
    "        value=model_ids[0],\n",
    "        description=\"Select Model:\",\n",
    "    )    \n",
    "    prompt_text = widgets.Text(\n",
    "        value=\"\",\n",
    "        placeholder=\"Enter your prompt\",\n",
    "        description=\"Prompt:\",\n",
    "    )    \n",
    "    num_images_slider = widgets.IntSlider(\n",
    "        value=2,\n",
    "        min=1,\n",
    "        max=10,\n",
    "        step=1,\n",
    "        description=\"Num of Images:\",\n",
    "    )    \n",
    "    enhance_checkbox = widgets.Checkbox(\n",
    "        value=False,\n",
    "        description=\"Auto enhance the prompt?\",\n",
    "        disabled=False,\n",
    "        indent=False\n",
    "    )\n",
    "    \n",
    "    layout = widgets.Layout(margin=\"0px 50px 10px 0px\")\n",
    "    button = widgets.Button(description=\"Generate Images!\")   \n",
    "    left_box = VBox([model_dropdown, prompt_text, num_images_slider], layout=layout)\n",
    "    right_box = VBox([enhance_checkbox], layout=layout)\n",
    "    user_input_widgets = HBox([left_box, right_box], layout=layout)  \n",
    "    display(user_input_widgets)\n",
    "    display(button)\n",
    "    display(out)\n",
    "\n",
    "    \n",
    "    def on_submit(button):\n",
    "        with out:\n",
    "            clear_output(wait=True)\n",
    "            print(\"\\nGenerating Images, once generated images will be saved `./output` dir, please wait...\")\n",
    "            selected_model_index = model_ids.index(model_dropdown.value)\n",
    "            model_id = model_ids[selected_model_index]\n",
    "            prompt = prompt_text.value\n",
    "            num_images = num_images_slider.value\n",
    "            model = Text2ImgModel(model_id, device=\"xpu\")\n",
    "            \n",
    "            enhancements = [\n",
    "                \"dark\",\n",
    "                \"purple light\",\n",
    "                \"dreaming\",\n",
    "                \"cyberpunk\",\n",
    "                \"ancient\" \", rustic\",\n",
    "                \"gothic\",\n",
    "                \"historical\",\n",
    "                \"punchy\",\n",
    "                \"photo\" \"vivid colors\",\n",
    "                \"4k\",\n",
    "                \"bright\",\n",
    "                \"exquisite\",\n",
    "                \"painting\",\n",
    "                \"art\",\n",
    "                \"fantasy [,/organic]\",\n",
    "                \"detailed\",\n",
    "                \"trending in artstation fantasy\",\n",
    "                \"electric\",\n",
    "                \"night\",\n",
    "            ]\n",
    "            \n",
    "            if enhance_checkbox.value:\n",
    "                prompt = prompt + \" \" + \" \".join(random.sample(enhancements, 5))\n",
    "                print(f\"Using enhanced prompt: {prompt}\")    \n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                model.generate_images(\n",
    "                    prompt,\n",
    "                    num_images=num_images,\n",
    "                    save_path=\"./output\",\n",
    "                )\n",
    "                clear_output(wait=True)\n",
    "                display_generated_images()\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nUser interrupted image generation...\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "            finally:\n",
    "                print(\n",
    "                    f\"Complete generating {num_images} images in './output' in {time.time() - start_time:.2f} seconds.\"\n",
    "                )\n",
    "                \n",
    "    button.on_click(on_submit)\n",
    "\n",
    "def display_generated_images(output_dir=\"output\"):\n",
    "    image_files = [f for f in os.listdir(output_dir) if f.endswith((\".png\", \".jpg\"))]    \n",
    "    num_images = len(image_files)\n",
    "    grid_size = int(np.ceil(np.sqrt(num_images)))\n",
    "    fig, axs = plt.subplots(grid_size, grid_size, figsize=(15, 15))    \n",
    "    if num_images == 1:\n",
    "        axs = np.array([[axs]])\n",
    "    elif grid_size == 1:\n",
    "        axs = np.array([axs])\n",
    "    for ax, image_file in zip(axs.ravel(), image_files):\n",
    "        img = mp_img.imread(os.path.join(output_dir, image_file))\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")  # Hide axes\n",
    "    for ax in axs.ravel()[num_images:]:\n",
    "        ax.axis(\"off\") \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dd723f-da20-4cb3-b5e3-fa8653661ebf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Let's Dive In! And Experience the Art of Creation**\n",
    "\n",
    "Eager to breathe life into your words? The interface below is your canvas. Set your preferences, type in your desired visual, and hit the \"Generate Images!\" button to witness the magic of Stable Diffusion.\n",
    "\n",
    "What unfolds are unique images sculpted by the power of your words, each a manifestation of the expansive interpretations our model draws from your input.\n",
    "\n",
    "This isn't just about AI; it's about the alchemy of your imagination combined with the prowess of Diffusion models. Every image stands as a symbol of the boundless potential of this synergy.\n",
    "\n",
    "So, why wait? Let's paint the canvas with your imagination!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df8d6b65-39b9-4472-b6a6-878e4c2cefdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0302327e3c324d368e01a2e6b1ce097d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Dropdown(description='Select Model:', options=('CompVis/stable-diffusion-v1-4', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1d91cc48984a19bc90adb878c32760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate Images!', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805458eda935421da206d9b7a9093ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c760991-214d-4990-8153-25b5dc032618",
   "metadata": {},
   "source": [
    "## Reference and Guidelines for Models Used in This Notebook\n",
    "\n",
    "### CompVis/stable-diffusion-v1-4\n",
    "- **Model card:** [CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4)\n",
    "- **License:** CreativeML OpenRAIL M license\n",
    "- **Reference:**\n",
    "    ```bibtex\n",
    "    @InProceedings{Rombach_2022_CVPR,\n",
    "        author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\n",
    "        title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n",
    "        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n",
    "        month     = {June},\n",
    "        year      = {2022},\n",
    "        pages     = {10684-10695}\n",
    "    }\n",
    "    ```\n",
    "\n",
    "### stabilityai/stable-diffusion-2\n",
    "- **Model card:** [stabilityai/stable-diffusion-2](https://huggingface.co/stabilityai/stable-diffusion-2)\n",
    "- **License:** CreativeML Open RAIL++-M License\n",
    "- **Reference:**\n",
    "    ```bibtex\n",
    "    @InProceedings{Rombach_2022_CVPR,\n",
    "        author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\n",
    "        title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n",
    "        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n",
    "        month     = {June},\n",
    "        year      = {2022},\n",
    "        pages     = {10684-10695}\n",
    "    }\n",
    "    ```\n",
    "\n",
    "### stable-diffusion-xl-base-1.0\n",
    "- **Model card:** [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n",
    "- **License:** CreativeML Open RAIL++-M License\n",
    "\n",
    "\n",
    "### Disclaimer for Using Stable Diffusion Models\n",
    "\n",
    "The stable diffusion models provided here are powerful tools for high-resolution image synthesis, including text-to-image and image-to-image transformations. While they are designed to produce high-quality results, users should be aware of potential limitations:\n",
    "\n",
    "- **Quality Variation:** The quality of generated images may vary based on the complexity of the input text or image, and the alignment with the model's training data.\n",
    "- **Resource Consumption:** These models may require significant computational resources, and performance might vary depending on the hardware used.\n",
    "- **Licensing and Usage Constraints:** Please carefully review the licensing information associated with each model to ensure compliance with all terms and conditions.\n",
    "- **Ethical Considerations:** Consider the ethical implications of the generated content, especially in contexts that may involve sensitive or controversial subjects.\n",
    "\n",
    "For detailed information on each model's capabilities, limitations, and best practices, please refer to the respective model cards and associated publications linked below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1f3a47-f6be-4ce1-8439-9e95bb1eb64a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion_env",
   "language": "python",
   "name": "diffusion_env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
