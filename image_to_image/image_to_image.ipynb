{
 "cells": [
  {
   "cell_type": "raw",
   "id": "bab46b17-057c-4981-88d7-ee5373b20bca",
   "metadata": {},
   "source": [
    "SPDX-License-Identifier: Apache-2.0\n",
    "Copyright (c) 2023, Rahul Unnikrishnan Nair <rahul.unnikrishnan.nair@intel.com>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a78a6c7-8d5d-4ac6-bb9b-93524a2a311d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Unleash Your Creativity: Image-to-Image Generation with Stable Diffusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d196333-054f-43e7-8c07-e5a65612b24d",
   "metadata": {},
   "source": [
    "Hello and welcome! Whether you're an artist, engineer, or simply someone curious about the blend of art and technology, this is the place for you.\n",
    "\n",
    "Imagine taking a picture and a few words of inspiration, and watching as they transform into a new visual representation. With Stable Diffusion, this imaginative exercise becomes a tangible experience. Want to see a landscape shift from day to night? Or perhaps turn a doodle into a detailed artwork? Dive in to explore these possibilities and more.\n",
    "\n",
    "Behind the scenes, we're harnessing the power of Intel® Data Center GPU Max Series GPUs. These GPUs are well-suited to handle the demands of tasks like Stable Diffusion, ensuring a smooth experience for you.\n",
    "\n",
    "This guide is designed to be straightforward and user-friendly. We'll introduce you to examples, help you understand the results, and offer ways to tweak the process. No deep technical knowledge required, just a spark of creativity and a dash of curiosity.\n",
    "\n",
    "Ready to get started? Let's begin by setting up a few things and diving into the experience!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0fe0f41a-baea-4cdf-ac53-be8700eea3e0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Required packages, install if not installed (assume PyTorch* and Intel® Extension for PyTorch* is already present\n",
    "#!pip install diffusers accelerate transformers validators ipywidgets tensorboardX\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for a cleaner output.\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import random\n",
    "import requests\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex  # adds xpu namespace to PyTorch, enabling you to use Intel GPUs\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "from diffusers import DPMSolverMultistepScheduler\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from typing import List, Dict, Tuple\n",
    "import validators\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12196920-265c-4390-9fb3-5f031e7ace2d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A Peek Under the Hood**\n",
    "\n",
    "For those curious about how all of this works, here\"s a deeper dive into the code. Don\"t worry if you\"re not tech-savvy; you don\"t need to understand this to use the notebook. But for those interested, let\"s explore:\n",
    "\n",
    "- **Class Definition**: We\"ve defined a class `Img2ImgModel` that does the heavy lifting. It sets up and optimizes the Stable Diffusion model for image transformations based on text prompts.\n",
    "  \n",
    "- **Initialization**: When creating an instance of this class, we can specify various parameters like the model\"s path, device to run on (defaulting to Intel\"s \"xpu\" device), and data type. We\"ve also included options to optimize the model for performance and warm it up for faster initial runs.\n",
    "  \n",
    "- **Pipeline Loading**: The `_load_pipeline` method sets up the necessary processes (or pipeline) for our image transformations using the specified pre-trained model.\n",
    "  \n",
    "- **Optimization**: For best performance, the model is optimized using Intel-specific techniques in the `_optimize_pipeline` and `optimize_pipeline` methods using Intel Extension For PyTorch.\n",
    "  \n",
    "- **Image Handling**: Methods like `get_image_from_url` allow for fetching and processing images directly from web URLs.\n",
    "  \n",
    "- **Model Warmup**: Before diving into the main tasks, we \"warm up\" the model using the `warmup_model` method. This ensures that subsequent runs are faster.\n",
    "  \n",
    "- **Image Generation**: The heart of this class, `generate_images`, takes in a text prompt and an image URL. It then generates new images based on the text\"s instructions, applying various parameters like strength of transformation and guidance scale.\n",
    "\n",
    "Feel free to explore the code below, and if you\"re inclined, you can see how we\"ve implemented and optimized it for Intel GPUs.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "31324959-3ade-40d7-bbf5-feb885d931f4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Img2ImgModel:\n",
    "    \"\"\"\n",
    "    This class creates a model for transforming images based on given prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id_or_path: str,\n",
    "        device: str = \"xpu\",\n",
    "        torch_dtype: torch.dtype = torch.bfloat16,\n",
    "        optimize: bool = True,\n",
    "        warmup: bool = True,\n",
    "        scheduler: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the model with the specified parameters.\n",
    "\n",
    "        Args:\n",
    "            model_id_or_path (str): The ID or path of the pre-trained model.\n",
    "            device (str, optional): The device to run the model on. Defaults to \"xpu\".\n",
    "            torch_dtype (torch.dtype, optional): The data type to use for the model. Defaults to torch.float16.\n",
    "            optimize (bool, optional): Whether to optimize the model. Defaults to True.\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.data_type = torch_dtype\n",
    "        self.scheduler = scheduler\n",
    "        self.generator = torch.Generator()  # .manual_seed(99)\n",
    "        self.pipeline = self._load_pipeline(model_id_or_path, torch_dtype)\n",
    "        if optimize:\n",
    "            start_time = time.time()\n",
    "            print(\"Optimizing the model...\")\n",
    "            self.optimize_pipeline()\n",
    "            print(\n",
    "                \"Optimization completed in {:.2f} seconds.\".format(\n",
    "                    time.time() - start_time\n",
    "                )\n",
    "            )\n",
    "        if warmup:\n",
    "            self.warmup_model()\n",
    "\n",
    "    def _load_pipeline(\n",
    "        self, model_id_or_path: str, torch_dtype: torch.dtype\n",
    "    ) -> StableDiffusionImg2ImgPipeline:\n",
    "        \"\"\"\n",
    "        Load the pipeline for the model.\n",
    "\n",
    "        Args:\n",
    "            model_id_or_path (str): The ID or path of the pre-trained model.\n",
    "            torch_dtype (torch.dtype): The data type to use for the model.\n",
    "\n",
    "        Returns:\n",
    "            StableDiffusionImg2ImgPipeline: The loaded pipeline.\n",
    "        \"\"\"\n",
    "        print(\"Loading the model...\")\n",
    "        pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "            model_id_or_path, torch_dtype=torch_dtype\n",
    "        )\n",
    "        pipeline = pipeline.to(self.device)\n",
    "        if self.scheduler:\n",
    "            pipeline.scheduler = DPMSolverMultistepScheduler.from_config(\n",
    "                pipeline.scheduler.config\n",
    "            )\n",
    "        print(\"Model loaded.\")\n",
    "        return pipeline\n",
    "\n",
    "    def _optimize_pipeline(\n",
    "        self, pipeline: StableDiffusionImg2ImgPipeline\n",
    "    ) -> StableDiffusionImg2ImgPipeline:\n",
    "        \"\"\"\n",
    "        Optimize the pipeline of the model.\n",
    "\n",
    "        Args:\n",
    "            pipeline (StableDiffusionImg2ImgPipeline): The pipeline to optimize.\n",
    "\n",
    "        Returns:\n",
    "            StableDiffusionImg2ImgPipeline: The optimized pipeline.\n",
    "        \"\"\"\n",
    "        for attr in dir(pipeline):\n",
    "            if isinstance(getattr(pipeline, attr), nn.Module):\n",
    "                setattr(\n",
    "                    pipeline,\n",
    "                    attr,\n",
    "                    ipex.optimize(\n",
    "                        getattr(pipeline, attr).eval(),\n",
    "                        dtype=pipeline.text_encoder.dtype,\n",
    "                        inplace=True,\n",
    "                    ),\n",
    "                )\n",
    "        return pipeline\n",
    "\n",
    "    def optimize_pipeline(self) -> None:\n",
    "        \"\"\"\n",
    "        Optimize the pipeline of the model.\n",
    "        \"\"\"\n",
    "        self.pipeline = self._optimize_pipeline(self.pipeline)\n",
    "\n",
    "    def get_image_from_url(self, url: str, path: str) -> Image.Image:\n",
    "        \"\"\"\n",
    "        Get an image from a URL or from a local path if it exists.\n",
    "\n",
    "        Args:\n",
    "            url (str): The URL of the image.\n",
    "            path (str): The local path of the image.\n",
    "\n",
    "        Returns:\n",
    "            Image.Image: The loaded image.\n",
    "        \"\"\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(\n",
    "                f\"Failed to download image. Status code: {response.status_code}\"\n",
    "            )\n",
    "        if not response.headers[\"content-type\"].startswith(\"image\"):\n",
    "            raise Exception(\n",
    "                f\"URL does not point to an image. Content type: {response.headers['content-type']}\"\n",
    "            )\n",
    "        img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "        img.save(path)\n",
    "        img = img.resize((768, 512))\n",
    "        return img\n",
    "\n",
    "    def warmup_model(self):\n",
    "        \"\"\"\n",
    "        Warms up the model by generating a sample image.\n",
    "        \"\"\"\n",
    "        print(\"Setting up model...\")\n",
    "        start_time = time.time()\n",
    "        image_url = \"https://user-images.githubusercontent.com/786476/256401499-f010e3f8-6f8d-4e9f-9d1f-178d3571e7b9.png\"\n",
    "        try:\n",
    "            self.generate_images(\n",
    "                image_url=image_url,\n",
    "                prompt=\"A beautiful day\",\n",
    "                num_images=1,\n",
    "                save_path=\"/tmp\",\n",
    "            )\n",
    "        except Exception:\n",
    "            print(\"model warmup delayed...\")\n",
    "        print(\n",
    "            \"Model is set up and ready! Warm-up completed in {:.2f} seconds.\".format(\n",
    "                time.time() - start_time\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def get_inputs(self, prompt, batch_size=1):\n",
    "        self.generator = [torch.Generator() for i in range(batch_size)]\n",
    "        prompts = batch_size * [prompt]\n",
    "        return {\"prompt\": prompts, \"generator\": self.generator}\n",
    "\n",
    "    def generate_images(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        image_url: str,\n",
    "        num_images: int = 5,\n",
    "        num_inference_steps: int = 30,\n",
    "        strength: float = 0.75,\n",
    "        guidance_scale: float = 7.5,\n",
    "        save_path: str = \"output\",\n",
    "        batch_size: int = 1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate images based on the provided prompt and variations.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The base prompt for the generation.\n",
    "            image_url (str): The URL of the seed image.\n",
    "            variations (List[str]): The list of variations to apply to the prompt.\n",
    "            num_images (int, optional): The number of images to generate. Defaults to 5.\n",
    "            num_inference_steps (int, optional): Number of noise removal steps.\n",
    "            strength (float, optional): The strength of the transformation. Defaults to 0.75.\n",
    "            guidance_scale (float, optional): The scale of the guidance. Defaults to 7.5.\n",
    "            save_path (str, optional): The path to save the generated images. Defaults to \"output\".\n",
    "\n",
    "        \"\"\"\n",
    "        input_image_path = \"input.png\"\n",
    "        init_image = self.get_image_from_url(image_url, input_image_path)\n",
    "        init_images = [init_image for _ in range(batch_size)]\n",
    "        for i in range(0, num_images, batch_size):\n",
    "            with torch.xpu.amp.autocast(\n",
    "                enabled=True if self.data_type != torch.float32 else False,\n",
    "                dtype=self.data_type,\n",
    "            ):\n",
    "                if batch_size > 1:\n",
    "                    inputs = self.get_inputs(batch_size=batch_size, prompt=prompt)\n",
    "                    images = self.pipeline(\n",
    "                        **inputs,\n",
    "                        image=init_images,\n",
    "                        strength=strength,\n",
    "                        guidance_scale=guidance_scale,\n",
    "                        num_inference_steps=num_inference_steps,\n",
    "                    ).images\n",
    "                else:\n",
    "                    images = self.pipeline(\n",
    "                        prompt=prompt,\n",
    "                        image=init_images,\n",
    "                        strength=strength,\n",
    "                        guidance_scale=guidance_scale,\n",
    "                        num_inference_steps=num_inference_steps,\n",
    "                    ).images\n",
    "\n",
    "                for j in range(len(images)):\n",
    "                    output_image_path = os.path.join(\n",
    "                        save_path,\n",
    "                        f\"{'_'.join(prompt.split()[:3])}_{i+j}.png\",\n",
    "                    )\n",
    "                    images[j].save(output_image_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478a838e-3032-4f4b-ac23-7ef142d20d77",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Setting Up the User Interface**\n",
    "\n",
    "In the next section, we\"ll craft an interactive user interface right here in the notebook. This will allow you to easily select a model, provide an image URL, type in a text prompt, and define other parameters without diving into the code itself.\n",
    "\n",
    "- **Model Selection**: Choose from available pre-trained models.\n",
    "- **Prompt Input**: Type in a text prompt to guide the image transformation.\n",
    "- **Number of Images**: Decide how many images you\"d like to generate.\n",
    "- **Image URL**: Provide a link to an online image or use the default provided.\n",
    "- **Enhancement**: Opt for auto-enhancements to the prompt for added creativity.\n",
    "\n",
    "Once you\"ve provided your preferences, a button click will initiate the magic!\n",
    "\n",
    "Let\"s set this up:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e012d657-7fe4-426e-bdeb-033701a38a52",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mp_img\n",
    "import validators\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "from IPython.display import Image as IPImage\n",
    "from ipywidgets import VBox, HBox\n",
    "\n",
    "def image_to_image():\n",
    "    out = widgets.Output()\n",
    "    output_dir = \"output\"\n",
    "    num_images = 2\n",
    "    model_ids = [\n",
    "        \"runwayml/stable-diffusion-v1-5\",\n",
    "        \"stabilityai/stable-diffusion-2-1\",\n",
    "    ]    \n",
    "    model_dropdown = widgets.Dropdown(\n",
    "        options=model_ids,\n",
    "        value=model_ids[0],\n",
    "        description=\"Select Model:\",\n",
    "    )    \n",
    "    prompt_text = widgets.Text(\n",
    "        value=\"\",\n",
    "        placeholder=\"Enter your prompt\",\n",
    "        description=\"Prompt:\",\n",
    "    )    \n",
    "    num_images_slider = widgets.IntSlider(\n",
    "        value=2,\n",
    "        min=1,\n",
    "        max=10,\n",
    "        step=1,\n",
    "        description=\"Num of Images:\",\n",
    "    )    \n",
    "    image_url_text = widgets.Text(\n",
    "        value=\"https://user-images.githubusercontent.com/786476/256401499-f010e3f8-6f8d-4e9f-9d1f-178d3571e7b9.png\",\n",
    "        placeholder=\"Enter an image URL\",\n",
    "        description=\"Image URL:\",\n",
    "    )\n",
    "    enhance_checkbox = widgets.Checkbox(\n",
    "        value=False,\n",
    "        description=\"Auto enhance the prompt?\",\n",
    "        disabled=False,\n",
    "        indent=False\n",
    "    )\n",
    "    layout = widgets.Layout(margin=\"0px 50px 10px 0px\")\n",
    "    button = widgets.Button(description=\"Generate Images!\")\n",
    "    left_box = VBox([model_dropdown, prompt_text, num_images_slider], layout=layout)\n",
    "    right_box = VBox([image_url_text, enhance_checkbox], layout=layout)\n",
    "    user_input_widgets = HBox([left_box, right_box], layout=layout)\n",
    "    display(user_input_widgets)\n",
    "    display(button)\n",
    "    display(out)\n",
    "    \n",
    "    \n",
    "    def on_submit(button):\n",
    "        clear_output(wait=True)\n",
    "        with out:\n",
    "            print(\"Generating Images, once generated images will be saved `./output` dir, please wait...\")\n",
    "            selected_model_index = model_ids.index(model_dropdown.value)\n",
    "            model_id = model_ids[selected_model_index]\n",
    "            prompt = prompt_text.value\n",
    "            num_images = num_images_slider.value\n",
    "            image_url = image_url_text.value\n",
    "            \n",
    "            if not validators.url(image_url):\n",
    "                print(\"The input is not a valid URL. Using the default URL instead.\")\n",
    "                image_url = \"https://user-images.githubusercontent.com/786476/256401499-f010e3f8-6f8d-4e9f-9d1f-178d3571e7b9.png\"       \n",
    "            model = Img2ImgModel(model_id, device=\"xpu\")\n",
    "            enhancements = [\n",
    "            \"purple light\",\n",
    "            \"dreaming\",\n",
    "            \"cyberpunk\",\n",
    "            \"ancient\" \", rustic\",\n",
    "            \"gothic\",\n",
    "            \"historical\",\n",
    "            \"punchy\",\n",
    "            \"photo\" \"vivid colors\",\n",
    "            \"4k\",\n",
    "            \"bright\",\n",
    "            \"exquisite\",\n",
    "            \"painting\",\n",
    "            \"art\",\n",
    "            \"fantasy [,/organic]\",\n",
    "            \"detailed\",\n",
    "            \"trending in artstation fantasy\",\n",
    "            \"electric\",\n",
    "            \"night\",\n",
    "            ]\n",
    "            \n",
    "            if enhance_checkbox.value:\n",
    "                prompt = prompt + \" \" + \" \".join(random.sample(enhancements, 5))\n",
    "                print(f\"Using enhanced prompt: {prompt}\")    \n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                model.generate_images(\n",
    "                    prompt=prompt,\n",
    "                    image_url=image_url,\n",
    "                    num_images=num_images,\n",
    "                )\n",
    "                clear_output(wait=True)\n",
    "                display_generated_images()\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nUser interrupted image generation...\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "            finally:\n",
    "                status = f\"Complete generating {num_images} images in {time.time() - start_time:.2f} seconds.\"\n",
    "                print(status)\n",
    "    button.on_click(on_submit)\n",
    "\n",
    "def display_generated_images(output_dir=\"output\"):\n",
    "    image_files = [f for f in os.listdir(output_dir) if f.endswith((\".png\", \".jpg\"))]    \n",
    "    num_images = len(image_files)\n",
    "    grid_size = int(np.ceil(np.sqrt(num_images)))\n",
    "    fig, axs = plt.subplots(grid_size, grid_size, figsize=(15, 15))    \n",
    "    if num_images == 1:\n",
    "        axs = np.array([[axs]])\n",
    "    elif grid_size == 1:\n",
    "        axs = np.array([axs])\n",
    "    for ax, image_file in zip(axs.ravel(), image_files):\n",
    "        img = mp_img.imread(os.path.join(output_dir, image_file))\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")  # Hide axes\n",
    "    for ax in axs.ravel()[num_images:]:\n",
    "        ax.axis(\"off\") \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da73551-7707-46ac-884f-0b0f6138280b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Let\"s Dive In! and Witness the Magi**\n",
    "\n",
    "Ready to generate some amazing images? Just interact with the user interface below. Once you\"ve set your preferences, click the \"Generate Images!\" button to witness the power of Stable Diffusion in action.\n",
    "\n",
    "Once done, you\"ll find the images generated based on your prompt. Presented in a grid format, they showcase the diverse interpretations and transformations the model inferred from your input.\n",
    "\n",
    "Take a moment to marvel at the fusion of your creativity and the prowess of Stable Diffusion. Each image is a testament to the limitless possibilities this technology offers.\n",
    "\n",
    "\n",
    "Go ahead, unleash your creativity!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39941920-7519-4dd3-8e59-37c8f1389dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611851bb20104425bb59010b8fdc2ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Dropdown(description='Select Model:', options=('runwayml/stable-diffusion-v1-5',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11486be6f0d430b882c980a8c805df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate Images!', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0510c65f054b62a424dcc401790a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1915e149-0ad6-475d-9b16-b34eb3d2806a",
   "metadata": {},
   "source": [
    "## Reference and Guidelines for Models Used in This Notebook\n",
    "\n",
    "\n",
    "### runwayml/stable-diffusion-v1-5\n",
    "- **Model card:** [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)\n",
    "- **License:** CreativeML OpenRAIL M license\n",
    "- **Reference:**\n",
    "    ```bibtex\n",
    "    @InProceedings{Rombach_2022_CVPR,\n",
    "        author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\n",
    "        title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n",
    "        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n",
    "        month     = {June},\n",
    "        year      = {2022},\n",
    "        pages     = {10684-10695}\n",
    "    }\n",
    "    ```\n",
    "\n",
    "### stabilityai/stable-diffusion-2-1\n",
    "- **Model card:** [stabilityai/stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1)\n",
    "- **License:** CreativeML Open RAIL++-M License\n",
    "- **Reference:**\n",
    "    ```bibtex\n",
    "    @InProceedings{Rombach_2022_CVPR,\n",
    "        author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\n",
    "        title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n",
    "        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n",
    "        month     = {June},\n",
    "        year      = {2022},\n",
    "        pages     = {10684-10695}\n",
    "    }\n",
    "    ```\n",
    "\n",
    "### Disclaimer for Using Stable Diffusion Models\n",
    "\n",
    "The stable diffusion models provided here are powerful tools for high-resolution image synthesis, including text-to-image and image-to-image transformations. While they are designed to produce high-quality results, users should be aware of potential limitations:\n",
    "\n",
    "- **Quality Variation:** The quality of generated images may vary based on the complexity of the input text or image, and the alignment with the model's training data.\n",
    "- **Licensing and Usage Constraints:** Please carefully review the licensing information associated with each model to ensure compliance with all terms and conditions.\n",
    "- **Ethical Considerations:** Consider the ethical implications of the generated content, especially in contexts that may involve sensitive or controversial subjects.\n",
    "\n",
    "For detailed information on each model's capabilities, limitations, and best practices, please refer to the respective model cards and associated publications linked above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa7fc1-df66-4e7f-8610-ed11e730759e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion_env",
   "language": "python",
   "name": "diffusion_env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
